{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndataset = pd.read_csv('/kaggle/input/flickr8k/captions.txt')\ndataset.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:13:37.982384Z","iopub.execute_input":"2025-02-21T02:13:37.982705Z","iopub.status.idle":"2025-02-21T02:13:38.053266Z","shell.execute_reply.started":"2025-02-21T02:13:37.982682Z","shell.execute_reply":"2025-02-21T02:13:38.052466Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                       image  \\\n0  1000268201_693b08cb0e.jpg   \n1  1000268201_693b08cb0e.jpg   \n2  1000268201_693b08cb0e.jpg   \n3  1000268201_693b08cb0e.jpg   \n4  1000268201_693b08cb0e.jpg   \n\n                                             caption  \n0  A child in a pink dress is climbing up a set o...  \n1              A girl going into a wooden building .  \n2   A little girl climbing into a wooden playhouse .  \n3  A little girl climbing the stairs to her playh...  \n4  A little girl in a pink dress going into a woo...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A child in a pink dress is climbing up a set o...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A girl going into a wooden building .</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing into a wooden playhouse .</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl climbing the stairs to her playh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1000268201_693b08cb0e.jpg</td>\n      <td>A little girl in a pink dress going into a woo...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import torch, torchvision \nfrom torchvision import transforms\nfrom PIL import Image\n\nimage_transforms = transforms.Compose([\n    # Resize the image to 256x256 pixels\n    transforms.Resize((256, 256)),\n    \n    # Optionally, you could add a random crop for data augmentation:\n    # transforms.RandomCrop(224),\n    \n    # Alternatively, center crop to 224x224\n    transforms.CenterCrop(224),\n    \n    # Convert image to tensor (scales pixel values to [0,1])\n    transforms.ToTensor(),\n    \n    # Normalize the image using ImageNet's mean and standard deviation\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                         std=[0.229, 0.224, 0.225])\n])\n\ntrain_data = []\nepoch = len(dataset)\n\ncnt = 0\n\nfor i in range(0, epoch, 1):\n    if i % 5 == 0:\n        cur_path = '/kaggle/input/flickr8k/Images/'\n        cur_img_path = dataset['image'][i]\n    \n        img_path = cur_path + cur_img_path\n    \n        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is in RGB\n    \n        preprocessed_image = image_transforms(image)\n\n    train_data.append([preprocessed_image, dataset['caption'][i]])\n    \nlen(train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:13:38.054358Z","iopub.execute_input":"2025-02-21T02:13:38.054727Z","iopub.status.idle":"2025-02-21T02:14:34.914295Z","shell.execute_reply.started":"2025-02-21T02:13:38.054684Z","shell.execute_reply":"2025-02-21T02:14:34.913420Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"40455"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"vocab = {}\nnum_words = 4\n\nvocab['<sos>'] = 0\nvocab['<eos>'] = 1\nvocab['<unk>'] = 2\nvocab['<pad>'] = 3\n\nfor img, sentence in train_data:\n    for word in sentence.split():\n        word = word.lower()\n        if word not in vocab:\n            vocab[word] = num_words\n            num_words += 1\n\nlen(vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:34.916228Z","iopub.execute_input":"2025-02-21T02:14:34.916462Z","iopub.status.idle":"2025-02-21T02:14:35.058872Z","shell.execute_reply.started":"2025-02-21T02:14:34.916443Z","shell.execute_reply":"2025-02-21T02:14:35.058140Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"8922"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"text_encodings = []\nimg_data = []\n\ndef encode(sentence, max_length):\n    output = []\n    output.append(vocab['<sos>'])\n    \n    for word in sentence.split():\n        output.append(vocab[word.lower()])\n        \n    output.append(vocab['<eos>'])\n    \n    for i in range(len(output), max_length):\n        output.append(vocab['<pad>'])\n\n    return output\n\n\nfor img, sentence in train_data:\n    img_data.append(img)\n    text_encodings.append(encode(sentence, 50))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:35.059877Z","iopub.execute_input":"2025-02-21T02:14:35.060111Z","iopub.status.idle":"2025-02-21T02:14:35.351279Z","shell.execute_reply.started":"2025-02-21T02:14:35.060091Z","shell.execute_reply":"2025-02-21T02:14:35.350270Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass ImageCaptioningDataset(Dataset):\n    def __init__(self, captions, inp):\n        self.inp = inp\n        self.captions = captions\n\n    def __len__(self):\n        return len(self.captions)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.inp[idx]), torch.tensor(self.captions[idx])\n\ntrain_idx = int(0.8 * len(train_data))\nval_idx = int(0.9 * len(train_data))\n\ntrain_dataset = ImageCaptioningDataset(text_encodings[:train_idx], img_data[:train_idx])\nval_dataset = ImageCaptioningDataset(text_encodings[train_idx : val_idx], img_data[train_idx : val_idx])\ntest_dataset = ImageCaptioningDataset(text_encodings[val_idx :], img_data[val_idx :])\n\ntype(train_dataset), train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:35.352357Z","iopub.execute_input":"2025-02-21T02:14:35.352667Z","iopub.status.idle":"2025-02-21T02:14:35.360940Z","shell.execute_reply.started":"2025-02-21T02:14:35.352637Z","shell.execute_reply":"2025-02-21T02:14:35.360136Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"(__main__.ImageCaptioningDataset,\n <__main__.ImageCaptioningDataset at 0x7b18eb775ed0>)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"BATCH_SIZE = 32 * 4\n\ntrain_dataloader = DataLoader(train_dataset, batch_size= BATCH_SIZE, shuffle= True)\nval_dataloader = DataLoader(val_dataset, batch_size= BATCH_SIZE, shuffle= True)\ntest_dataloader = DataLoader(test_dataset, batch_size= BATCH_SIZE, shuffle= True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:35.361861Z","iopub.execute_input":"2025-02-21T02:14:35.362163Z","iopub.status.idle":"2025-02-21T02:14:35.379328Z","shell.execute_reply.started":"2025-02-21T02:14:35.362137Z","shell.execute_reply":"2025-02-21T02:14:35.378612Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from torch import nn\nfrom torchvision import models\nimport torch.nn.init as init\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, embed_size):\n        super().__init__()\n        resnet = models.resnet50(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n\n        modules = list(resnet.children())[:-1]\n        self.resnet = nn.Sequential(*modules)\n        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n        self.batch = nn.BatchNorm1d(embed_size, momentum=0.01)\n        \n        # Use nn.init functions to initialize parameters safely.\n        init.normal_(self.embed.weight, mean=0.0, std=0.02)\n        init.constant_(self.embed.bias, 0)\n\n    def forward(self, images):\n        features = self.resnet(images)\n        features = features.view(features.size(0), -1)\n        features = self.batch(self.embed(features))\n        return features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:35.380366Z","iopub.execute_input":"2025-02-21T02:14:35.380644Z","iopub.status.idle":"2025-02-21T02:14:35.386831Z","shell.execute_reply.started":"2025-02-21T02:14:35.380623Z","shell.execute_reply":"2025-02-21T02:14:35.386135Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"embed_size = 256\nvocab_size = len(vocab)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ndummy_inp = torch.randn([5, 3, 224, 224])\nencoder = EncoderCNN(embed_size)\nencoder.eval()\noutput = encoder(dummy_inp)\noutput.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:35.387659Z","iopub.execute_input":"2025-02-21T02:14:35.387850Z","iopub.status.idle":"2025-02-21T02:14:36.308535Z","shell.execute_reply.started":"2025-02-21T02:14:35.387833Z","shell.execute_reply":"2025-02-21T02:14:36.307559Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"torch.Size([5, 256])"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"class DecoderRNN(nn.Module):\n  def __init__(self, embed_size, hidden_size, vocab_size):\n    super(DecoderRNN, self).__init__()\n    self.embed = nn.Embedding(vocab_size, embed_size)\n    self.lstm = nn.LSTM(embed_size, hidden_size)\n    self.linear = nn.Linear(hidden_size, vocab_size)\n    self.dropout = nn.Dropout(0.5)\n    \n  def forward(self, features, captions):\n    embeddings = self.dropout(self.embed(captions))\n    embeddings = embeddings.transpose(0, 1)\n    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n    \n    hiddens, _ = self.lstm(embeddings)\n    outputs = self.linear(hiddens)\n      \n    return outputs\n\n# Instantiate the decoder\ndecoder = DecoderRNN(embed_size, 256, vocab_size)\n\n# Correctly create dummy_inp2 using torch.randint:\ndummy_inp2 = torch.randint(0, vocab_size, (5, 50))\n\nt_output = decoder(output, dummy_inp2)\n\nt_output.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:36.311323Z","iopub.execute_input":"2025-02-21T02:14:36.311590Z","iopub.status.idle":"2025-02-21T02:14:36.367108Z","shell.execute_reply.started":"2025-02-21T02:14:36.311560Z","shell.execute_reply":"2025-02-21T02:14:36.366146Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([51, 5, 8922])"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"class Encoder_Decoder(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size):\n        super().__init__()\n        self.CNN = EncoderCNN(embed_size)\n        self.RNN = DecoderRNN(embed_size, hidden_size, vocab_size)\n\n    def forward(self, images, captions):\n        features = self.CNN(images)\n        output = self.RNN(features, captions)\n\n        return output\n\n    def sample(self, images, max_len=50):\n        features = self.CNN(images)  # shape: (batch, embed_size)\n        batch_size = features.size(0)\n        sampled_ids = []  # to store predicted word indices\n\n        inputs = features.unsqueeze(0)  # shape: (1, batch, embed_size)\n        states = None\n\n        for t in range(max_len):\n            hiddens, states = self.RNN.lstm(inputs, states)  # hiddens: (1, batch, hidden_size)\n            \n            logits = self.RNN.linear(hiddens.squeeze(0))      # shape: (batch, vocab_size)\n            \n            predicted = logits.argmax(dim=1)                  # shape: (batch,)\n            sampled_ids.append(predicted)\n\n            inputs = self.RNN.embed(predicted)                # shape: (batch, embed_size)\n            inputs = inputs.unsqueeze(0)                      # shape: (1, batch, embed_size)\n\n        sampled_ids = torch.stack(sampled_ids, dim=1)  # shape: (batch, max_len)\n        return sampled_ids\n\nmodel = Encoder_Decoder(embed_size, 256, vocab_size).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:36.368692Z","iopub.execute_input":"2025-02-21T02:14:36.368962Z","iopub.status.idle":"2025-02-21T02:14:36.907510Z","shell.execute_reply.started":"2025-02-21T02:14:36.368941Z","shell.execute_reply":"2025-02-21T02:14:36.906814Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from torch import optim\nimport torch.nn as nn\nimport torch\n\ndef evaluate_loss(model, val_dataloader, criterion, device):\n    model.eval()\n    total_loss = 0.0\n    with torch.no_grad():\n        for images, captions in val_dataloader:\n            images = images.to(device)\n            captions = captions.to(device)\n            \n            # Prepare input and target captions\n            input_captions = captions[:, :-1]\n            target_captions = captions[:, 1:]\n            \n            outputs = model(images, input_captions)\n            outputs = outputs[1:].transpose(0, 1)  # shape: (batch, seq_len, vocab_size)\n            \n            loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n                             target_captions.reshape(-1))\n            total_loss += loss.item()\n    \n    avg_loss = total_loss / len(val_dataloader)\n    model.train()  # back to training mode if needed\n    return avg_loss\n    \ndef train(model, train_dataloader, epochs, lr, device):\n    criterion = nn.CrossEntropyLoss(ignore_index=3)\n    \n    params = list(model.RNN.parameters()) + list(model.CNN.embed.parameters()) + list(model.CNN.batch.parameters())\n    \n    optimizer = optim.Adam(params, lr=lr)\n    \n    model.train() \n    for epoch in range(epochs):\n        total_loss = 0.0\n        for batch_idx, (images, captions) in enumerate(train_dataloader):\n            images = images.to(device)         # shape: (batch, 3, 224, 224)\n            captions = captions.to(device)       # shape: (batch, seq_len)\n            \n            input_captions = captions[:, :-1]    # shape: (batch, seq_len-1)\n            target_captions = captions[:, 1:]      # shape: (batch, seq_len-1)\n            \n            optimizer.zero_grad()\n            \n            outputs = model(images, input_captions)\n            #print(outputs.shape)\n            outputs = outputs[1:]  # shape now: (seq_len, batch, vocab_size)\n            \n            outputs = outputs.transpose(0, 1)\n            \n            loss = criterion(outputs.reshape(-1, outputs.size(-1)),\n                             target_captions.reshape(-1))\n            \n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            if (batch_idx + 1) % 10 == 0:\n                print(f\"Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n\n        val_loss = evaluate_loss(model, val_dataloader, nn.CrossEntropyLoss(ignore_index=vocab['<pad>']), device)\n        print(f\"Validation Loss: {val_loss:.4f}\")\n        \n        avg_loss = total_loss / len(train_dataloader)\n        print(f\"Epoch [{epoch+1}/{epochs}] Average Loss: {avg_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:36.908330Z","iopub.execute_input":"2025-02-21T02:14:36.908631Z","iopub.status.idle":"2025-02-21T02:14:36.917757Z","shell.execute_reply.started":"2025-02-21T02:14:36.908603Z","shell.execute_reply":"2025-02-21T02:14:36.916934Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train(model, train_dataloader, 15, 1e-3, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:14:36.918597Z","iopub.execute_input":"2025-02-21T02:14:36.918913Z","iopub.status.idle":"2025-02-21T02:55:18.455265Z","shell.execute_reply.started":"2025-02-21T02:14:36.918884Z","shell.execute_reply":"2025-02-21T02:55:18.454444Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-17-b5cc2ac53f57>:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(self.inp[idx]), torch.tensor(self.captions[idx])\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/15], Step [10/253], Loss: 8.2417\nEpoch [1/15], Step [20/253], Loss: 5.4260\nEpoch [1/15], Step [30/253], Loss: 5.1128\nEpoch [1/15], Step [40/253], Loss: 4.8478\nEpoch [1/15], Step [50/253], Loss: 4.8278\nEpoch [1/15], Step [60/253], Loss: 4.6696\nEpoch [1/15], Step [70/253], Loss: 4.4515\nEpoch [1/15], Step [80/253], Loss: 4.3581\nEpoch [1/15], Step [90/253], Loss: 4.4524\nEpoch [1/15], Step [100/253], Loss: 4.2672\nEpoch [1/15], Step [110/253], Loss: 4.1993\nEpoch [1/15], Step [120/253], Loss: 4.1561\nEpoch [1/15], Step [130/253], Loss: 4.1819\nEpoch [1/15], Step [140/253], Loss: 4.1793\nEpoch [1/15], Step [150/253], Loss: 3.9672\nEpoch [1/15], Step [160/253], Loss: 3.9160\nEpoch [1/15], Step [170/253], Loss: 3.8892\nEpoch [1/15], Step [180/253], Loss: 3.7159\nEpoch [1/15], Step [190/253], Loss: 4.0411\nEpoch [1/15], Step [200/253], Loss: 3.9622\nEpoch [1/15], Step [210/253], Loss: 3.8126\nEpoch [1/15], Step [220/253], Loss: 3.7644\nEpoch [1/15], Step [230/253], Loss: 3.8020\nEpoch [1/15], Step [240/253], Loss: 3.8509\nEpoch [1/15], Step [250/253], Loss: 3.7241\nValidation Loss: 3.7117\nEpoch [1/15] Average Loss: 4.4765\nEpoch [2/15], Step [10/253], Loss: 3.6385\nEpoch [2/15], Step [20/253], Loss: 3.6248\nEpoch [2/15], Step [30/253], Loss: 3.7242\nEpoch [2/15], Step [40/253], Loss: 3.5072\nEpoch [2/15], Step [50/253], Loss: 3.6400\nEpoch [2/15], Step [60/253], Loss: 3.6270\nEpoch [2/15], Step [70/253], Loss: 3.6337\nEpoch [2/15], Step [80/253], Loss: 3.6756\nEpoch [2/15], Step [90/253], Loss: 3.5066\nEpoch [2/15], Step [100/253], Loss: 3.5839\nEpoch [2/15], Step [110/253], Loss: 3.5036\nEpoch [2/15], Step [120/253], Loss: 3.4290\nEpoch [2/15], Step [130/253], Loss: 3.4968\nEpoch [2/15], Step [140/253], Loss: 3.7217\nEpoch [2/15], Step [150/253], Loss: 3.6240\nEpoch [2/15], Step [160/253], Loss: 3.6113\nEpoch [2/15], Step [170/253], Loss: 3.5018\nEpoch [2/15], Step [180/253], Loss: 3.4536\nEpoch [2/15], Step [190/253], Loss: 3.3481\nEpoch [2/15], Step [200/253], Loss: 3.4048\nEpoch [2/15], Step [210/253], Loss: 3.4392\nEpoch [2/15], Step [220/253], Loss: 3.3921\nEpoch [2/15], Step [230/253], Loss: 3.4944\nEpoch [2/15], Step [240/253], Loss: 3.4826\nEpoch [2/15], Step [250/253], Loss: 3.5205\nValidation Loss: 3.4035\nEpoch [2/15] Average Loss: 3.5109\nEpoch [3/15], Step [10/253], Loss: 3.2094\nEpoch [3/15], Step [20/253], Loss: 3.3760\nEpoch [3/15], Step [30/253], Loss: 3.1287\nEpoch [3/15], Step [40/253], Loss: 3.2302\nEpoch [3/15], Step [50/253], Loss: 3.1835\nEpoch [3/15], Step [60/253], Loss: 3.3161\nEpoch [3/15], Step [70/253], Loss: 3.0586\nEpoch [3/15], Step [80/253], Loss: 3.3270\nEpoch [3/15], Step [90/253], Loss: 3.3369\nEpoch [3/15], Step [100/253], Loss: 3.3554\nEpoch [3/15], Step [110/253], Loss: 3.2715\nEpoch [3/15], Step [120/253], Loss: 3.2292\nEpoch [3/15], Step [130/253], Loss: 3.3294\nEpoch [3/15], Step [140/253], Loss: 3.0975\nEpoch [3/15], Step [150/253], Loss: 2.9920\nEpoch [3/15], Step [160/253], Loss: 3.0838\nEpoch [3/15], Step [170/253], Loss: 3.1658\nEpoch [3/15], Step [180/253], Loss: 3.1269\nEpoch [3/15], Step [190/253], Loss: 3.2030\nEpoch [3/15], Step [200/253], Loss: 3.1451\nEpoch [3/15], Step [210/253], Loss: 3.2107\nEpoch [3/15], Step [220/253], Loss: 3.0407\nEpoch [3/15], Step [230/253], Loss: 3.1043\nEpoch [3/15], Step [240/253], Loss: 3.1365\nEpoch [3/15], Step [250/253], Loss: 3.1611\nValidation Loss: 3.2214\nEpoch [3/15] Average Loss: 3.2121\nEpoch [4/15], Step [10/253], Loss: 3.1006\nEpoch [4/15], Step [20/253], Loss: 3.1270\nEpoch [4/15], Step [30/253], Loss: 3.0232\nEpoch [4/15], Step [40/253], Loss: 3.1183\nEpoch [4/15], Step [50/253], Loss: 2.9505\nEpoch [4/15], Step [60/253], Loss: 3.1263\nEpoch [4/15], Step [70/253], Loss: 3.1172\nEpoch [4/15], Step [80/253], Loss: 3.0496\nEpoch [4/15], Step [90/253], Loss: 2.9028\nEpoch [4/15], Step [100/253], Loss: 3.0895\nEpoch [4/15], Step [110/253], Loss: 3.0424\nEpoch [4/15], Step [120/253], Loss: 2.8276\nEpoch [4/15], Step [130/253], Loss: 2.9888\nEpoch [4/15], Step [140/253], Loss: 3.0932\nEpoch [4/15], Step [150/253], Loss: 2.9278\nEpoch [4/15], Step [160/253], Loss: 3.0354\nEpoch [4/15], Step [170/253], Loss: 3.0806\nEpoch [4/15], Step [180/253], Loss: 2.9887\nEpoch [4/15], Step [190/253], Loss: 3.0395\nEpoch [4/15], Step [200/253], Loss: 2.9600\nEpoch [4/15], Step [210/253], Loss: 3.0874\nEpoch [4/15], Step [220/253], Loss: 2.8433\nEpoch [4/15], Step [230/253], Loss: 3.0015\nEpoch [4/15], Step [240/253], Loss: 2.9465\nEpoch [4/15], Step [250/253], Loss: 2.8934\nValidation Loss: 3.1198\nEpoch [4/15] Average Loss: 3.0154\nEpoch [5/15], Step [10/253], Loss: 3.0994\nEpoch [5/15], Step [20/253], Loss: 3.0437\nEpoch [5/15], Step [30/253], Loss: 3.0111\nEpoch [5/15], Step [40/253], Loss: 2.8514\nEpoch [5/15], Step [50/253], Loss: 3.0296\nEpoch [5/15], Step [60/253], Loss: 2.8117\nEpoch [5/15], Step [70/253], Loss: 2.9546\nEpoch [5/15], Step [80/253], Loss: 2.9259\nEpoch [5/15], Step [90/253], Loss: 2.8306\nEpoch [5/15], Step [100/253], Loss: 2.9246\nEpoch [5/15], Step [110/253], Loss: 2.8581\nEpoch [5/15], Step [120/253], Loss: 2.9000\nEpoch [5/15], Step [130/253], Loss: 2.7981\nEpoch [5/15], Step [140/253], Loss: 2.8774\nEpoch [5/15], Step [150/253], Loss: 2.8101\nEpoch [5/15], Step [160/253], Loss: 2.7423\nEpoch [5/15], Step [170/253], Loss: 2.9374\nEpoch [5/15], Step [180/253], Loss: 2.8534\nEpoch [5/15], Step [190/253], Loss: 2.7669\nEpoch [5/15], Step [200/253], Loss: 2.9428\nEpoch [5/15], Step [210/253], Loss: 2.9556\nEpoch [5/15], Step [220/253], Loss: 2.8844\nEpoch [5/15], Step [230/253], Loss: 2.9432\nEpoch [5/15], Step [240/253], Loss: 2.8974\nEpoch [5/15], Step [250/253], Loss: 2.8846\nValidation Loss: 3.0610\nEpoch [5/15] Average Loss: 2.8740\nEpoch [6/15], Step [10/253], Loss: 2.8495\nEpoch [6/15], Step [20/253], Loss: 2.7265\nEpoch [6/15], Step [30/253], Loss: 2.8175\nEpoch [6/15], Step [40/253], Loss: 2.7537\nEpoch [6/15], Step [50/253], Loss: 2.6552\nEpoch [6/15], Step [60/253], Loss: 2.7315\nEpoch [6/15], Step [70/253], Loss: 2.7723\nEpoch [6/15], Step [80/253], Loss: 2.9497\nEpoch [6/15], Step [90/253], Loss: 2.7471\nEpoch [6/15], Step [100/253], Loss: 2.7234\nEpoch [6/15], Step [110/253], Loss: 2.7172\nEpoch [6/15], Step [120/253], Loss: 2.8863\nEpoch [6/15], Step [130/253], Loss: 2.8483\nEpoch [6/15], Step [140/253], Loss: 2.6825\nEpoch [6/15], Step [150/253], Loss: 2.7697\nEpoch [6/15], Step [160/253], Loss: 2.9335\nEpoch [6/15], Step [170/253], Loss: 2.7316\nEpoch [6/15], Step [180/253], Loss: 2.7750\nEpoch [6/15], Step [190/253], Loss: 2.7211\nEpoch [6/15], Step [200/253], Loss: 2.7439\nEpoch [6/15], Step [210/253], Loss: 2.8084\nEpoch [6/15], Step [220/253], Loss: 2.6856\nEpoch [6/15], Step [230/253], Loss: 2.7338\nEpoch [6/15], Step [240/253], Loss: 2.7946\nEpoch [6/15], Step [250/253], Loss: 2.7575\nValidation Loss: 3.0198\nEpoch [6/15] Average Loss: 2.7610\nEpoch [7/15], Step [10/253], Loss: 2.7634\nEpoch [7/15], Step [20/253], Loss: 2.6458\nEpoch [7/15], Step [30/253], Loss: 2.8158\nEpoch [7/15], Step [40/253], Loss: 2.6329\nEpoch [7/15], Step [50/253], Loss: 2.6549\nEpoch [7/15], Step [60/253], Loss: 2.6495\nEpoch [7/15], Step [70/253], Loss: 2.7667\nEpoch [7/15], Step [80/253], Loss: 2.5909\nEpoch [7/15], Step [90/253], Loss: 2.6302\nEpoch [7/15], Step [100/253], Loss: 2.6486\nEpoch [7/15], Step [110/253], Loss: 2.6933\nEpoch [7/15], Step [120/253], Loss: 2.7624\nEpoch [7/15], Step [130/253], Loss: 2.6097\nEpoch [7/15], Step [140/253], Loss: 2.5846\nEpoch [7/15], Step [150/253], Loss: 2.6915\nEpoch [7/15], Step [160/253], Loss: 2.6700\nEpoch [7/15], Step [170/253], Loss: 2.7320\nEpoch [7/15], Step [180/253], Loss: 2.6986\nEpoch [7/15], Step [190/253], Loss: 2.6288\nEpoch [7/15], Step [200/253], Loss: 2.6978\nEpoch [7/15], Step [210/253], Loss: 2.4595\nEpoch [7/15], Step [220/253], Loss: 2.5895\nEpoch [7/15], Step [230/253], Loss: 2.6201\nEpoch [7/15], Step [240/253], Loss: 2.7040\nEpoch [7/15], Step [250/253], Loss: 2.7258\nValidation Loss: 2.9972\nEpoch [7/15] Average Loss: 2.6697\nEpoch [8/15], Step [10/253], Loss: 2.6659\nEpoch [8/15], Step [20/253], Loss: 2.4701\nEpoch [8/15], Step [30/253], Loss: 2.5319\nEpoch [8/15], Step [40/253], Loss: 2.6092\nEpoch [8/15], Step [50/253], Loss: 2.6865\nEpoch [8/15], Step [60/253], Loss: 2.4743\nEpoch [8/15], Step [70/253], Loss: 2.5375\nEpoch [8/15], Step [80/253], Loss: 2.6466\nEpoch [8/15], Step [90/253], Loss: 2.6699\nEpoch [8/15], Step [100/253], Loss: 2.6277\nEpoch [8/15], Step [110/253], Loss: 2.4978\nEpoch [8/15], Step [120/253], Loss: 2.6902\nEpoch [8/15], Step [130/253], Loss: 2.5696\nEpoch [8/15], Step [140/253], Loss: 2.6184\nEpoch [8/15], Step [150/253], Loss: 2.5175\nEpoch [8/15], Step [160/253], Loss: 2.5684\nEpoch [8/15], Step [170/253], Loss: 2.7165\nEpoch [8/15], Step [180/253], Loss: 2.5814\nEpoch [8/15], Step [190/253], Loss: 2.6662\nEpoch [8/15], Step [200/253], Loss: 2.5733\nEpoch [8/15], Step [210/253], Loss: 2.7545\nEpoch [8/15], Step [220/253], Loss: 2.7118\nEpoch [8/15], Step [230/253], Loss: 2.4896\nEpoch [8/15], Step [240/253], Loss: 2.5748\nEpoch [8/15], Step [250/253], Loss: 2.4843\nValidation Loss: 2.9804\nEpoch [8/15] Average Loss: 2.5876\nEpoch [9/15], Step [10/253], Loss: 2.3800\nEpoch [9/15], Step [20/253], Loss: 2.6599\nEpoch [9/15], Step [30/253], Loss: 2.4657\nEpoch [9/15], Step [40/253], Loss: 2.5986\nEpoch [9/15], Step [50/253], Loss: 2.4871\nEpoch [9/15], Step [60/253], Loss: 2.4755\nEpoch [9/15], Step [70/253], Loss: 2.5747\nEpoch [9/15], Step [80/253], Loss: 2.5632\nEpoch [9/15], Step [90/253], Loss: 2.4856\nEpoch [9/15], Step [100/253], Loss: 2.5621\nEpoch [9/15], Step [110/253], Loss: 2.5456\nEpoch [9/15], Step [120/253], Loss: 2.5735\nEpoch [9/15], Step [130/253], Loss: 2.6452\nEpoch [9/15], Step [140/253], Loss: 2.5263\nEpoch [9/15], Step [150/253], Loss: 2.5027\nEpoch [9/15], Step [160/253], Loss: 2.5574\nEpoch [9/15], Step [170/253], Loss: 2.4947\nEpoch [9/15], Step [180/253], Loss: 2.4673\nEpoch [9/15], Step [190/253], Loss: 2.4823\nEpoch [9/15], Step [200/253], Loss: 2.5275\nEpoch [9/15], Step [210/253], Loss: 2.5739\nEpoch [9/15], Step [220/253], Loss: 2.4100\nEpoch [9/15], Step [230/253], Loss: 2.4997\nEpoch [9/15], Step [240/253], Loss: 2.4478\nEpoch [9/15], Step [250/253], Loss: 2.5275\nValidation Loss: 2.9586\nEpoch [9/15] Average Loss: 2.5156\nEpoch [10/15], Step [10/253], Loss: 2.4272\nEpoch [10/15], Step [20/253], Loss: 2.3905\nEpoch [10/15], Step [30/253], Loss: 2.4346\nEpoch [10/15], Step [40/253], Loss: 2.4970\nEpoch [10/15], Step [50/253], Loss: 2.4558\nEpoch [10/15], Step [60/253], Loss: 2.5105\nEpoch [10/15], Step [70/253], Loss: 2.4350\nEpoch [10/15], Step [80/253], Loss: 2.5115\nEpoch [10/15], Step [90/253], Loss: 2.4200\nEpoch [10/15], Step [100/253], Loss: 2.5183\nEpoch [10/15], Step [110/253], Loss: 2.3589\nEpoch [10/15], Step [120/253], Loss: 2.4256\nEpoch [10/15], Step [130/253], Loss: 2.4825\nEpoch [10/15], Step [140/253], Loss: 2.3868\nEpoch [10/15], Step [150/253], Loss: 2.4304\nEpoch [10/15], Step [160/253], Loss: 2.4563\nEpoch [10/15], Step [170/253], Loss: 2.4196\nEpoch [10/15], Step [180/253], Loss: 2.4064\nEpoch [10/15], Step [190/253], Loss: 2.6508\nEpoch [10/15], Step [200/253], Loss: 2.5105\nEpoch [10/15], Step [210/253], Loss: 2.4742\nEpoch [10/15], Step [220/253], Loss: 2.3924\nEpoch [10/15], Step [230/253], Loss: 2.5262\nEpoch [10/15], Step [240/253], Loss: 2.4148\nEpoch [10/15], Step [250/253], Loss: 2.3925\nValidation Loss: 2.9551\nEpoch [10/15] Average Loss: 2.4500\nEpoch [11/15], Step [10/253], Loss: 2.2591\nEpoch [11/15], Step [20/253], Loss: 2.3994\nEpoch [11/15], Step [30/253], Loss: 2.3697\nEpoch [11/15], Step [40/253], Loss: 2.3988\nEpoch [11/15], Step [50/253], Loss: 2.2659\nEpoch [11/15], Step [60/253], Loss: 2.3574\nEpoch [11/15], Step [70/253], Loss: 2.4293\nEpoch [11/15], Step [80/253], Loss: 2.4560\nEpoch [11/15], Step [90/253], Loss: 2.4005\nEpoch [11/15], Step [100/253], Loss: 2.3923\nEpoch [11/15], Step [110/253], Loss: 2.3267\nEpoch [11/15], Step [120/253], Loss: 2.3601\nEpoch [11/15], Step [130/253], Loss: 2.3780\nEpoch [11/15], Step [140/253], Loss: 2.4222\nEpoch [11/15], Step [150/253], Loss: 2.3648\nEpoch [11/15], Step [160/253], Loss: 2.4242\nEpoch [11/15], Step [170/253], Loss: 2.2549\nEpoch [11/15], Step [180/253], Loss: 2.3890\nEpoch [11/15], Step [190/253], Loss: 2.3236\nEpoch [11/15], Step [200/253], Loss: 2.3782\nEpoch [11/15], Step [210/253], Loss: 2.3564\nEpoch [11/15], Step [220/253], Loss: 2.3758\nEpoch [11/15], Step [230/253], Loss: 2.3742\nEpoch [11/15], Step [240/253], Loss: 2.4250\nEpoch [11/15], Step [250/253], Loss: 2.3335\nValidation Loss: 2.9548\nEpoch [11/15] Average Loss: 2.3907\nEpoch [12/15], Step [10/253], Loss: 2.2656\nEpoch [12/15], Step [20/253], Loss: 2.1934\nEpoch [12/15], Step [30/253], Loss: 2.3278\nEpoch [12/15], Step [40/253], Loss: 2.4085\nEpoch [12/15], Step [50/253], Loss: 2.3687\nEpoch [12/15], Step [60/253], Loss: 2.3440\nEpoch [12/15], Step [70/253], Loss: 2.3756\nEpoch [12/15], Step [80/253], Loss: 2.3524\nEpoch [12/15], Step [90/253], Loss: 2.2353\nEpoch [12/15], Step [100/253], Loss: 2.3123\nEpoch [12/15], Step [110/253], Loss: 2.3168\nEpoch [12/15], Step [120/253], Loss: 2.3794\nEpoch [12/15], Step [130/253], Loss: 2.2856\nEpoch [12/15], Step [140/253], Loss: 2.3003\nEpoch [12/15], Step [150/253], Loss: 2.3081\nEpoch [12/15], Step [160/253], Loss: 2.2316\nEpoch [12/15], Step [170/253], Loss: 2.4233\nEpoch [12/15], Step [180/253], Loss: 2.3381\nEpoch [12/15], Step [190/253], Loss: 2.3897\nEpoch [12/15], Step [200/253], Loss: 2.4338\nEpoch [12/15], Step [210/253], Loss: 2.3294\nEpoch [12/15], Step [220/253], Loss: 2.2684\nEpoch [12/15], Step [230/253], Loss: 2.3762\nEpoch [12/15], Step [240/253], Loss: 2.2662\nEpoch [12/15], Step [250/253], Loss: 2.4016\nValidation Loss: 2.9615\nEpoch [12/15] Average Loss: 2.3382\nEpoch [13/15], Step [10/253], Loss: 2.2436\nEpoch [13/15], Step [20/253], Loss: 2.2895\nEpoch [13/15], Step [30/253], Loss: 2.2300\nEpoch [13/15], Step [40/253], Loss: 2.1666\nEpoch [13/15], Step [50/253], Loss: 2.3002\nEpoch [13/15], Step [60/253], Loss: 2.1523\nEpoch [13/15], Step [70/253], Loss: 2.2628\nEpoch [13/15], Step [80/253], Loss: 2.2898\nEpoch [13/15], Step [90/253], Loss: 2.3110\nEpoch [13/15], Step [100/253], Loss: 2.3009\nEpoch [13/15], Step [110/253], Loss: 2.4424\nEpoch [13/15], Step [120/253], Loss: 2.2524\nEpoch [13/15], Step [130/253], Loss: 2.2494\nEpoch [13/15], Step [140/253], Loss: 2.3022\nEpoch [13/15], Step [150/253], Loss: 2.2889\nEpoch [13/15], Step [160/253], Loss: 2.3702\nEpoch [13/15], Step [170/253], Loss: 2.3228\nEpoch [13/15], Step [180/253], Loss: 2.2649\nEpoch [13/15], Step [190/253], Loss: 2.3060\nEpoch [13/15], Step [200/253], Loss: 2.3051\nEpoch [13/15], Step [210/253], Loss: 2.3554\nEpoch [13/15], Step [220/253], Loss: 2.3894\nEpoch [13/15], Step [230/253], Loss: 2.2949\nEpoch [13/15], Step [240/253], Loss: 2.2331\nEpoch [13/15], Step [250/253], Loss: 2.2623\nValidation Loss: 2.9609\nEpoch [13/15] Average Loss: 2.2877\nEpoch [14/15], Step [10/253], Loss: 2.1056\nEpoch [14/15], Step [20/253], Loss: 2.2455\nEpoch [14/15], Step [30/253], Loss: 2.1419\nEpoch [14/15], Step [40/253], Loss: 2.2096\nEpoch [14/15], Step [50/253], Loss: 2.2392\nEpoch [14/15], Step [60/253], Loss: 2.3193\nEpoch [14/15], Step [70/253], Loss: 2.3115\nEpoch [14/15], Step [80/253], Loss: 2.2614\nEpoch [14/15], Step [90/253], Loss: 2.2408\nEpoch [14/15], Step [100/253], Loss: 2.2650\nEpoch [14/15], Step [110/253], Loss: 2.2748\nEpoch [14/15], Step [120/253], Loss: 2.2476\nEpoch [14/15], Step [130/253], Loss: 2.2441\nEpoch [14/15], Step [140/253], Loss: 2.3220\nEpoch [14/15], Step [150/253], Loss: 2.2216\nEpoch [14/15], Step [160/253], Loss: 2.2674\nEpoch [14/15], Step [170/253], Loss: 2.3496\nEpoch [14/15], Step [180/253], Loss: 2.3674\nEpoch [14/15], Step [190/253], Loss: 2.2465\nEpoch [14/15], Step [200/253], Loss: 2.3109\nEpoch [14/15], Step [210/253], Loss: 2.2910\nEpoch [14/15], Step [220/253], Loss: 2.3189\nEpoch [14/15], Step [230/253], Loss: 2.2606\nEpoch [14/15], Step [240/253], Loss: 2.3140\nEpoch [14/15], Step [250/253], Loss: 2.1922\nValidation Loss: 2.9634\nEpoch [14/15] Average Loss: 2.2429\nEpoch [15/15], Step [10/253], Loss: 2.1138\nEpoch [15/15], Step [20/253], Loss: 2.1388\nEpoch [15/15], Step [30/253], Loss: 2.1723\nEpoch [15/15], Step [40/253], Loss: 2.2881\nEpoch [15/15], Step [50/253], Loss: 2.1326\nEpoch [15/15], Step [60/253], Loss: 2.1678\nEpoch [15/15], Step [70/253], Loss: 2.1394\nEpoch [15/15], Step [80/253], Loss: 2.1811\nEpoch [15/15], Step [90/253], Loss: 2.1682\nEpoch [15/15], Step [100/253], Loss: 2.1512\nEpoch [15/15], Step [110/253], Loss: 2.2592\nEpoch [15/15], Step [120/253], Loss: 2.1937\nEpoch [15/15], Step [130/253], Loss: 2.2777\nEpoch [15/15], Step [140/253], Loss: 2.2951\nEpoch [15/15], Step [150/253], Loss: 2.1777\nEpoch [15/15], Step [160/253], Loss: 2.2189\nEpoch [15/15], Step [170/253], Loss: 2.1191\nEpoch [15/15], Step [180/253], Loss: 2.2706\nEpoch [15/15], Step [190/253], Loss: 2.1557\nEpoch [15/15], Step [200/253], Loss: 2.2221\nEpoch [15/15], Step [210/253], Loss: 2.2691\nEpoch [15/15], Step [220/253], Loss: 2.2504\nEpoch [15/15], Step [230/253], Loss: 2.1987\nEpoch [15/15], Step [240/253], Loss: 2.1775\nEpoch [15/15], Step [250/253], Loss: 2.2478\nValidation Loss: 2.9693\nEpoch [15/15] Average Loss: 2.2025\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"model.eval()\nimg = img_data[0]\nimg = img.to(device)\nimg = img.unsqueeze(0)  # Now shape becomes [1, 3, 224, 224]\nimg.shape\nimg_output = model.sample(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:55:18.456203Z","iopub.execute_input":"2025-02-21T02:55:18.456514Z","iopub.status.idle":"2025-02-21T02:55:18.617515Z","shell.execute_reply.started":"2025-02-21T02:55:18.456484Z","shell.execute_reply":"2025-02-21T02:55:18.616540Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"img_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T02:55:18.618547Z","iopub.execute_input":"2025-02-21T02:55:18.618834Z","iopub.status.idle":"2025-02-21T02:55:18.629001Z","shell.execute_reply.started":"2025-02-21T02:55:18.618807Z","shell.execute_reply":"2025-02-21T02:55:18.628138Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"tensor([[  4,  19,   6,   4,   7,   8,   9, 146,   6,  60,  13,   4,  22, 500,\n          18,   1,   1,  18,   1,   1,   1,  18,   1,   1,  18,   1,   1,  18,\n           1,   1,  18,   1,   1,  18,   1,   1,  18,   1,   1,  18,   1,   1,\n          18,   1,   1,  18,   1,   1,  18,   1]], device='cuda:0')"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"sentence = []\nimg_output = img_output.squeeze()\n\nidx_to_words = {}\nfor key, val in vocab.items():\n    idx_to_words[val] = key\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T03:17:24.042729Z","iopub.execute_input":"2025-02-21T03:17:24.043044Z","iopub.status.idle":"2025-02-21T03:17:24.050148Z","shell.execute_reply.started":"2025-02-21T03:17:24.043023Z","shell.execute_reply":"2025-02-21T03:17:24.049300Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"for element in img_output.tolist():\n    sentence.append(idx_to_words[element])\n\nsentence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T03:18:47.603645Z","iopub.execute_input":"2025-02-21T03:18:47.603968Z","iopub.status.idle":"2025-02-21T03:18:47.610368Z","shell.execute_reply.started":"2025-02-21T03:18:47.603941Z","shell.execute_reply":"2025-02-21T03:18:47.609579Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['a',\n 'girl',\n 'in',\n 'a',\n 'pink',\n 'dress',\n 'is',\n 'standing',\n 'in',\n 'front',\n 'of',\n 'a',\n 'wooden',\n 'door',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>',\n '<eos>',\n '.',\n '<eos>']"},"metadata":{}}],"execution_count":37}]}